{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,5)\n",
    "\n",
    "# Для кириллицы на графиках\n",
    "font = {'family': 'Verdana',\n",
    "        'weight': 'normal'}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "try:\n",
    "    from ipywidgets import interact, IntSlider, fixed, FloatSlider\n",
    "except ImportError:\n",
    "    print u'Так надо'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/header.png\"></center>\n",
    "\n",
    "<h1><center>Алгоритмы интеллектуальной обработки больших объемов данных</center></h1>\n",
    "<hr>\n",
    "<h2><center>SVM, Kernel Trick</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Quiz Time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## В предыдущий раз..\n",
    "\n",
    "Мы разговаривали про линейные модели\n",
    "\n",
    "* Рассмотрели линейную регрессию\n",
    "* Рассмотрели логистическую регрессию\n",
    "* Метод градиентного спуска для поиска коэффициентов моделей\n",
    "* Изучили регуляризацию в линейных моделях\n",
    "\n",
    "<center><img src='https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Svm_separating_hyperplanes_%28SVG%29.svg/512px-Svm_separating_hyperplanes_%28SVG%29.svg.png' width=400></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Про геометрию линейных моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Уравнение прямой задаётся как: $$g(x) = w_0 + w_1x_1 + w_2x_2 = w_0 + \\langle w, x \\rangle = w_0 +  w^\\top x $$\n",
    "\n",
    "* Если $g(x^*) > 0$, то $y^* = \\text{'черный'} = +1$\n",
    "* Если $g(x^*) < 0$, то $y^* = \\text{'белый'} = -1$\n",
    "* Если $g(x^*) = 0$, то мы находимся на линии\n",
    "* т.е. решающее правило: $y^* = sign(g(x^*))$\n",
    "\n",
    "Некоторые геометрические особенности\n",
    "* $\\frac{|g(x)|}{||w||}$ - расстояние от точки $x$ до гиперплоскости, степень \"уверенности\" в классификациий\n",
    "* Величину $M = y(\\langle w, x \\rangle + w_0) = y \\cdot g(x)$ называют **отступом**(margin)\n",
    "\n",
    "Если для какого-то объекта $M \\geq 0$, то его классификация выполнена успешно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$L(w) = \\sum_i [y^{(i)} (\\langle w, x^{(i)} \\rangle + w_0) < 0] \\rightarrow \\min_{w, w_0}$$\n",
    "<center><img src='http://jaquesgrobler.github.io/Online-Scikit-Learn-stat-tut/_images/plot_sgd_loss_functions_11.png' width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Линейноразделимый случай с двумя классами\n",
    "* Заметим что $g(x) = w_0 + \\langle w, x \\rangle$ и $g'(x) = c \\cdot (w_0 + \\langle w, x \\rangle)$ задают одну и ту же гиперплоскость\n",
    "* Подберем $c$ таким образом, чтобы $\\min\\limits_i M_i = \\min\\limits_i y \\cdot g(x_i) = 1$\n",
    "<center><img src='./images/margin.png'></center>\n",
    "* Таким образом выполняются следующие неравенства:\n",
    "    * $w_0 + \\langle w, x_i \\rangle \\geq 1$, если $y_i = + 1$\n",
    "    * $w_0 + \\langle w, x_i \\rangle \\leq - 1$, если $y_i = - 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Разделяющая полоса:  $ -1 \\leq w_0 + \\langle w, x \\rangle \\leq +1$\n",
    "* Ширина разделяющей полосы:\n",
    " $$\\langle (x_{+} -  x_{-}) , \\frac{w}{||w||}\\rangle = \\frac{\\langle w, x_{+} \\rangle - \\langle w, x_{-} \\rangle }{||w||} = \\frac{2}{||w||}  \\rightarrow \\max$$\n",
    " \n",
    " \n",
    "* Таким образом мы придем к оптимизационной задаче:\n",
    "$$\n",
    "\\begin{cases} \n",
    "   \\frac{1}{2} ||w||^2  \\rightarrow \\min  \\\\\n",
    "   y^{(i)}(\\langle w, x^{(i)} \\rangle + w_0 ) \\geq 1 \\quad i=1\\dots n\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "По теореме Куна-Таккера:\n",
    "\n",
    "\n",
    "\\begin{cases} \n",
    "   \\mathcal{L}(w,w_0,\\lambda) = \\frac{1}{2} ||w||^2  - \\sum\\limits_i \\lambda_i \\left( y^{(i)}(\\langle w, x \\rangle + w_0 )  - 1\\right)  \\rightarrow \\min\\limits_{w,w_0}\\max\\limits_{\\lambda}  \\\\\n",
    "   \\lambda_i \\geq 0 \\quad i=1\\dots n\\\\\n",
    "   \\lambda_i = 0 \\text{, либо }  \\langle w, x^{(i)} \\rangle + w_0 = y^{(i)} \\quad i=1\\dots n\n",
    "\\end{cases}\n",
    "Объекты, для которых  $\\lambda_i \\neq 0$ называются ** опорными ** \n",
    "\n",
    "\n",
    "Необходимое условие:\n",
    "*  $\\frac{\\partial \\mathcal{L} }{\\partial w} = w - \\sum\\limits_i \\lambda_iy_ix_i = 0 \\quad \\Rightarrow  \\quad w = \\sum\\limits_i \\lambda_iy_ix_i$\n",
    "*  $\\frac{\\partial \\mathcal{L} }{\\partial w_0} = \\sum\\limits_i \\lambda_iy_i = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Если подставить  эти результаты в $\\mathcal{L}$ то получится\n",
    "\\begin{cases}\n",
    "\\mathcal{L}(\\lambda) = \\sum\\limits_i\\lambda_i  - \\frac{1}{2} \\sum\\limits_i\\sum\\limits_j \\lambda_i \\lambda_j  y_i y_j (\\langle x_i, x_j \\rangle)  \\rightarrow \\max\\limits_\\lambda  \\\\\n",
    "\\lambda_i \\geq 0 \\quad i=1\\dots n \\\\\n",
    "\\sum\\limits_i \\lambda_iy_i = 0\n",
    "\\end{cases}\n",
    "\n",
    "* **Зависит не от самих объектов, а от их скалярного произведения! **\n",
    "* $\\mathcal{L}(\\lambda)$ - выпуклая и ограниченная сверху функция.\n",
    "* Имеем единственное решение при линейной разделимости\n",
    "* Находим $\\lambda_i,$ из $w = \\sum\\limits_i \\lambda_iy_ix_i$ находим коэффициенты $w$.\n",
    "* Свободный член $w_0$ определяется как среднее или медиана $\\{\\langle w, x_i \\rangle - y_i: \\lambda_i \\neq 0\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_classification\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def plot_svc_log_decision_function(clf1, clf2, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    x = np.linspace(plt.xlim()[0], plt.xlim()[1], 30)\n",
    "    y = np.linspace(plt.ylim()[0], plt.ylim()[1], 30)\n",
    "    XX, YY = np.meshgrid(x, y)\n",
    "    XY = np.c_[XX.ravel(), YY.ravel()]\n",
    "    P1 = clf1.decision_function(XY)\n",
    "    P1 = P1.reshape(XX.shape)\n",
    "    \n",
    "    P2 = clf2.decision_function(XY)\n",
    "    P2 = P2.reshape(XX.shape)\n",
    "    # plot the margins\n",
    "    cplot = ax.contour(XX, YY, P1, colors='k', label='svm',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    ax.clabel(cplot, inline=1, fontsize=10)\n",
    "    \n",
    "    ax.contour(XX, YY, P2, colors='r', label='logreg',\n",
    "               levels=[0], alpha=0.5,\n",
    "               linestyles=['-'])\n",
    "\n",
    "    \n",
    "def plot_svc_decision_function(clf1, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    x = np.linspace(plt.xlim()[0], plt.xlim()[1], 30)\n",
    "    y = np.linspace(plt.ylim()[0], plt.ylim()[1], 30)\n",
    "    XX, YY = np.meshgrid(x, y)\n",
    "    XY = np.c_[XX.ravel(), YY.ravel()]\n",
    "    P1 = clf1.decision_function(XY)\n",
    "    P1 = P1.reshape(XX.shape)\n",
    "    \n",
    "    # plot the margins\n",
    "    cplot = ax.contour(XX, YY, P1, colors='k', label='svm',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    ax.clabel(cplot, inline=1, fontsize=10)\n",
    "    \n",
    "\n",
    "def lin_sep_svm_demo(class_sep=2):\n",
    "    X, y = make_classification(n_samples=100, n_features=2, n_informative=2, class_sep=class_sep, scale=1,\n",
    "                                n_redundant=0, n_clusters_per_class=1, random_state=31)\n",
    "    # x_line = np.linspace(np.min(X) - 0.5, np.max(X) + 0.5)\n",
    "\n",
    "    lin_svm = SVC(kernel='linear', C=100).fit(X, y)\n",
    "    \n",
    "    log_reg = LogisticRegression(C=100).fit(X, y)\n",
    "    \n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=70, cmap='autumn')\n",
    "    plot_svc_log_decision_function(lin_svm, log_reg)\n",
    "    plt.scatter(lin_svm.support_vectors_[:, 0], lin_svm.support_vectors_[:, 1],\n",
    "            s=200, facecolors='none')\n",
    "    \n",
    "    \n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    \n",
    "    plt.xlim(-2, 5)\n",
    "    plt.ylim(-3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "interact(lin_sep_svm_demo, class_sep=FloatSlider(min=0.4, max=4, step=0.1, value=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Неразделимый случай \n",
    "\n",
    "Будем допускать пропуск объектов за разделительную линию\n",
    "* Вместо условия $y^{(i)}(\\langle w, x^{(i)} \\rangle + w_0 ) \\geq 1$\n",
    "* Будет условие $y^{(i)}(\\langle w, x^{(i)} \\rangle + w_0 ) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0$\n",
    "\n",
    "<center><img src='./images/slack.png'></center>\n",
    "\n",
    "А целевой функционал заменим на \n",
    "\n",
    "$$ \\frac{1}{2} ||w||^2 + C\\sum\\limits_i\\xi_i  \\rightarrow \\min\\limits_{w,w_0,\\xi}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Таким образом мы придем к оптимизационной задаче:\n",
    "$$\n",
    "\\begin{cases} \n",
    "   \\frac{1}{2} ||w||^2 + C\\sum\\limits_i\\xi_i  \\rightarrow \\min\\limits_{w,w_0,\\xi} \\\\\n",
    "   y^{(i)}(\\langle w, x^{(i)} \\rangle + w_0 ) \\geq 1 - \\xi_i \\quad i=1\\dots n \\\\\n",
    "   \\xi_i \\geq 0 \\quad i=1\\dots n\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Условия Куна-Таккера, необходимые условия оптимума $\\rightarrow$ получаем сопреженную задачу\n",
    "\\begin{cases}\n",
    "\\mathcal{L}(\\lambda) = \\sum\\limits_i\\lambda_i  - \\frac{1}{2} \\sum\\limits_i\\sum\\limits_j \\lambda_i \\lambda_j  y_i y_j (\\langle x_i, x_j \\rangle)  \\rightarrow \\max\\limits_\\lambda  \\\\\n",
    "0 \\leq \\lambda_i \\leq C \\quad i=1\\dots n \\\\\n",
    "\\sum\\limits_i \\lambda_iy_i = 0\n",
    "\\end{cases}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Заметим, что изначальный целевой функционал\n",
    "$$ \\frac{1}{2} ||w||^2 + C\\sum\\limits_i\\xi_i  \\rightarrow \\min\\limits_{w,w_0,\\xi}  $$\n",
    "Можно представить в виде\n",
    "$$ \\frac{1}{2С} ||w||^2 + \\sum\\limits_i(1-M_i)_+ \\rightarrow \\min\\limits_{w,w_0}, $$\n",
    "где $M_i$ - это отступ объекта  $x^{(i)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def lin_sep_svm_demo_C(class_sep=2, C=10):\n",
    "    X, y = make_classification(n_samples=100, n_features=2, n_informative=2, class_sep=class_sep, scale=1,\n",
    "                                n_redundant=0, n_clusters_per_class=1, random_state=31)\n",
    "    # x_line = np.linspace(np.min(X) - 0.5, np.max(X) + 0.5)\n",
    "\n",
    "    lin_svm = SVC(kernel='linear', C=C).fit(X, y)\n",
    "    \n",
    "    log_reg = LogisticRegression(C=C).fit(X, y)\n",
    "    \n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=70, cmap='autumn')\n",
    "    plot_svc_log_decision_function(lin_svm, log_reg)\n",
    "    plt.scatter(lin_svm.support_vectors_[:, 0], lin_svm.support_vectors_[:, 1],\n",
    "            s=200, facecolors='none')\n",
    "    \n",
    "    \n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    \n",
    "    plt.xlim(-2, 5)\n",
    "    plt.ylim(-3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "interact(lin_sep_svm_demo_C, class_sep=FloatSlider(min=0.2, max=4, value=2, step=0.2), C=FloatSlider(min=0.002, max=10, step=0.002, value=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ядра и спрямляющие пространства"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/interstellar.jpg' width='600'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_circles\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "def demo_nonlin_data():\n",
    "    X, y = make_circles(n_samples=100, factor=0.1, \n",
    "                        noise=0.1, random_state=0)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=70, cmap='autumn')\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    \n",
    "    r = X[:, 0] ** 2 + X[:, 1] ** 2\n",
    "    \n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    \n",
    "    ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=70, cmap='autumn')\n",
    "    ax.view_init(elev=30, azim=30)\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    ax.set_zlabel('$x_1^2 + x_2^2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "demo_nonlin_data() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* $\\psi: X \\rightarrow H$\n",
    "* $H$ - пространство большей размерности, в котором классы становятся линейноразделимыми называется **спрямляющим**.\n",
    "* Разделяющся гиперплоскость в таком пространстве будет линейной, но при проекции на исходное пространство $X$ - нет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Представим, что мы строим SVM в $H$\n",
    "\\begin{cases}\n",
    "\\mathcal{L}(\\lambda) = \\sum\\limits_i\\lambda_i  - \\frac{1}{2} \\sum\\limits_i\\sum\\limits_j \\lambda_i \\lambda_j  y_i y_j (\\langle \\psi(x_i), \\psi(x_j) \\rangle)  \\rightarrow \\max\\limits_\\lambda  \\\\\n",
    "0 \\leq \\lambda_i \\leq C \\quad i=1\\dots n \\\\\n",
    "\\sum\\limits_i \\lambda_iy_i = 0\n",
    "\\end{cases}\n",
    "* Получается, нам не нужно в явном виде выражать преобразование $\\psi$, нужно лишь знать результат скалярного произведения!\n",
    "* Kernel trick!\n",
    "* $k(x_i, x_j) = \\langle \\psi(x_i), \\psi(x_j) \\rangle$ - ядро"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Функция принятия решений:**\n",
    "* Было $g(x) = w^\\top x + w_0, \\quad w = \\sum\\limits_{i:\\lambda_i>0} \\lambda_iy_ix_i$\n",
    "* Получаем $g(x) = \\sum\\limits_{i:\\lambda_i>0} \\lambda_iy_i\\langle x_i, x \\rangle + w_0 = \\sum\\limits_{i:\\lambda_i>0} \\lambda_iy_i k(x_i, x) + w_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Теорема Мерсера **<br/>\n",
    "Функция $k(u, v)$ является ядром тогда и только тогда, когда она симметрична $k(u, v) = k(v, u)$, и неотрицательно определена, $ \\int_X \\int_X k(u,v) f(u)f(v) du dv \\geq 0$,  $\\forall f: X \\rightarrow \\mathbb{R}$\n",
    "\n",
    "** Некоторые способы построения ядер **\n",
    "1. Скалярное произведение - ядро, $k(u,v) = \\langle u, v \\rangle$\n",
    "2. Константа - ядро, $k(u,v) = 1$\n",
    "3. $\\forall \\phi: X \\rightarrow \\mathbb{R}$ произведение $k(u,v) =  \\phi(u) \\phi(v)$ - ядро\n",
    "4. $k(u,v)  = \\alpha_1k_1(u,v)  + \\alpha_2k_2(u,v) $ - ядро\n",
    "4. $k(u,v)  =  k_1(u,v) \\cdot k_2(u,v) $ - ядро\n",
    "5. $\\forall \\phi: X \\rightarrow X \\quad k(u,v) = k'(\\phi(u),\\phi(v)) $ - ядро"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Пример ядра\n",
    "* Пусть $X \\in \\mathbb{R}^2$\n",
    "* Рассмотрим $\\langle x, y \\rangle^2 = (x_1y_1 + x_2y_2)^2 = x_1y_1^2 + x_2y_2^2 + 2x_1y_1x_2y_2 $\n",
    "* $\\psi(x) = ?$\n",
    "* $k(x,y) = ?$\n",
    "\n",
    "#### Связь ядра, нормы и расстояния\n",
    "* $k(x,y)$ - ядро\n",
    "* Норма $\\|\\psi(x)\\|^2 = \\langle \\psi(x), \\psi(x) \\rangle = k(x,x)$\n",
    "* Расстояние $\\|\\psi(x) - \\psi(y)\\|^2 = \\langle \\psi(x)-\\psi(y), \\psi(x)-\\psi(y) \\rangle = k(x,x) + k(y,y) - 2k(x,y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Наиболее популярны следующие ядра:\n",
    "\n",
    "1. Линейное (linear): $$\\langle x, y\\rangle$$\n",
    "2. Полиномиальное (polynomial): $$(\\gamma \\langle x, y\\rangle + с)^d,$$ \n",
    "3. Radial basis function kernel (rbf): $$e^{(-\\gamma  \\cdot |x - y|^2)},$$ \n",
    "4. Sigmoid: $$\\tanh(\\gamma \\langle x,y \\rangle + r)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def lin_sep_svm_demo_kernel_C(class_sep=2, kernel='linear', C = 1, gamma=1.2, degree=2, coef0=0.0):\n",
    "    X, y = make_classification(n_samples=100, n_features=2, n_informative=2, class_sep=class_sep, scale=1,\n",
    "                                n_redundant=0, n_clusters_per_class=1, random_state=31)\n",
    "    # x_line = np.linspace(np.min(X) - 0.5, np.max(X) + 0.5)\n",
    "\n",
    "    lin_svm = SVC(kernel=kernel, C=C, gamma=gamma, degree=degree, coef0=0.0).fit(X, y)\n",
    "    \n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=70, cmap='autumn')\n",
    "    plot_svc_decision_function(lin_svm)\n",
    "    plt.scatter(lin_svm.support_vectors_[:, 0], lin_svm.support_vectors_[:, 1],\n",
    "            s=200, facecolors='none')\n",
    "    \n",
    "    \n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    \n",
    "    plt.xlim(-2, 5)\n",
    "    plt.ylim(-3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('3liCbRZPrZA', width=640, height=480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "interact(lin_sep_svm_demo_kernel_C, class_sep=FloatSlider(min=0.2, max=4, value=2, step=0.2), kernel=['rbf', 'linear', 'poly'],         coef0=FloatSlider(min=0, max=10, step=0.2, value=0),         C=FloatSlider(min=0.002, max=10, step=0.002, value=1),         degree=FloatSlider(min=2, max=3, step=1, value=2),         gamma=FloatSlider(min=0.01, max=5, step=0.01, value=0.5),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM для регрессии\n",
    "\n",
    "\n",
    "<center><img src='images/regression.png' width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# SVM для регрессии\n",
    "\n",
    "Переменные $\\xi_i \\geq 0$, $\\hat \\xi_i \\geq 0$ (slacks):\n",
    "$$\n",
    "y_i \\leq g(x_i) + \\epsilon + \\xi_i\n",
    "$$\n",
    "$$\n",
    "y_i \\geq g(x_i) - \\epsilon - \\hat \\xi_i\n",
    "$$\n",
    "Задача оптимизации\n",
    "\n",
    "$$\n",
    "\\begin{cases} \n",
    "   C \\sum_{i=1}^n (\\hat \\xi_i + \\xi_i) + \\frac{1}{2}\\|w\\|^2 \\rightarrow \\min\\limits_{w, w_0} \\\\\n",
    "   g(x_i) - y_i \\leq \\epsilon + \\xi_i \\\\\n",
    "   y_i - g(x_i) \\leq \\epsilon + \\hat{\\xi}_i \n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Вопросы?\n",
    "\n",
    "## Оставьте, пожалуйста, свой отзыв"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "livereveal": {
   "theme": "serif",
   "transition": "concave",
   "width": "1024px"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "none",
   "toc_window_display": false
  },
  "toc_position": {
   "height": "973px",
   "left": "0px",
   "right": "1708px",
   "top": "109px",
   "width": "212px"
  },
  "widgets": {
   "state": {
    "1e51dfc872e5446eb2bfbba243bef5f8": {
     "views": [
      {
       "cell_index": 32
      }
     ]
    },
    "38899fdf431c4dbc8e12478b2b6903d6": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "a8a4e100d2ac44a5ba089a52e3224d28": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
